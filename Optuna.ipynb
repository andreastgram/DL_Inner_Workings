{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyYAML\n",
      "  Using cached PyYAML-6.0-cp39-cp39-macosx_11_0_arm64.whl (173 kB)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: tqdm in /Users/andreas/.local/lib/python3.9/site-packages (from optuna) (4.64.1)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting alembic>=1.5.0\n",
      "  Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/andreas/opt/anaconda3/envs/transformers/lib/python3.9/site-packages (from optuna) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andreas/opt/anaconda3/envs/transformers/lib/python3.9/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /Users/andreas/opt/anaconda3/envs/transformers/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.2.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/andreas/opt/anaconda3/envs/transformers/lib/python3.9/site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/andreas/opt/anaconda3/envs/transformers/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
      "Installing collected packages: sqlalchemy, PyYAML, Mako, colorlog, cmaes, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 PyYAML-6.0 alembic-1.10.2 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0 sqlalchemy-2.0.7\n"
     ]
    }
   ],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "\n",
    "from idlmam import train_simple_network, Flatten, weight_reset, set_seed, run_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    train_subset = int(len(train_data)*0.8)\n",
    "    test_subset = len(train_data)-train_subset\n",
    "    \n",
    "    split = torch.utils.data.random_split(train_data, [train_subset, test_subset])\n",
    "    \n",
    "    t_loader = DataLoader(split[0], batch_size=B, shuffle=True)\n",
    "    v_loader = DataLoader(split[1], batch_size=B, shuffle=False)\n",
    "\n",
    "    #Hidden layer size\n",
    "    n = trial.suggest_int('neurons_per_layer', 16, 256) \n",
    "    layers = trial.suggest_int('hidden_layers', 1, 6) \n",
    "    #How many channels are in the input?\n",
    "    C = 1\n",
    "    #How many classes are there?\n",
    "    classes = 10\n",
    "\n",
    "    #At least one hidden layer, that take in D inputs\n",
    "    sequential_layers = [\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(D,  n),\n",
    "        nn.Tanh(),\n",
    "    ]\n",
    "    #Now lets add in a variable number of hidden layers, depending on what Optuna gave us for the \"layers\" parameter\n",
    "    for _ in range(layers-1):\n",
    "        sequential_layers.append( nn.Linear(n,  n) )\n",
    "        sequential_layers.append( nn.Tanh() )\n",
    "    \n",
    "    #Output layer\n",
    "    sequential_layers.append( nn.Linear(n, classes) )\n",
    "    \n",
    "    #Now turn the list of layers into a PyTorch Sequential Module \n",
    "    fc_model = nn.Sequential(*sequential_layers)\n",
    "    #What should our global learning rate be? Notice that we can ask for new hyper-parameters from optuna whenever we want.\n",
    "    eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(fc_model.parameters(), lr=eta_global)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs//3)\n",
    "    results = train_network(fc_model, loss_func, t_loader, test_loader=v_loader,\n",
    "                                     epochs=10, optimizer=optimizer, lr_schedule=scheduler,\n",
    "                                     score_funcs={'Accuracy': accuracy_score}, device=device, \n",
    "                                     disable_tqdm=True)\n",
    "\n",
    "    return results['test Accuracy'].iloc[-1]  # A objective value linked with the Trial object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10) #Normally we would do more like 50-100 trials,\n",
    "#but we are doing less to make sure this notebook runs in a reasonable amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_slice(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = optuna.visualization.plot_contour(study, params=['neurons_per_layer', 'hidden_layers', \"learning_rate\"])\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prunable studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectivePrunable(trial):\n",
    "    \n",
    "    train_subset = int(len(train_data)*0.8)\n",
    "    test_subset = len(train_data)-train_subset\n",
    "    \n",
    "    split = torch.utils.data.random_split(train_data, [train_subset, test_subset])\n",
    "    \n",
    "    t_loader = DataLoader(split[0], batch_size=B, shuffle=True)\n",
    "    v_loader = DataLoader(split[1], batch_size=B, shuffle=False)\n",
    "\n",
    "    #Hidden layer size\n",
    "    n = trial.suggest_int('neurons_per_layer', 1, 256) \n",
    "    layers = trial.suggest_int('hidden_layers', 1, 6) \n",
    "    #How many channels are in the input?\n",
    "    C = 1\n",
    "    #How many classes are there?\n",
    "    classes = 10\n",
    "\n",
    "    #At least one hidden layer, that take in D inputs\n",
    "    sequential_layers = [\n",
    "        Flatten(),\n",
    "        nn.Linear(D,  n),\n",
    "        nn.Tanh(),\n",
    "    ]\n",
    "    \n",
    "    for _ in range(layers-1):\n",
    "        sequential_layers.append( nn.Linear(n,  n) )\n",
    "        sequential_layers.append( nn.Tanh() )\n",
    "    \n",
    "    #Output layer\n",
    "    sequential_layers.append( nn.Linear(n, classes) )\n",
    "    \n",
    "\n",
    "    fc_model = nn.Sequential(*sequential_layers)\n",
    "    \n",
    "    eta_global = trial.suggest_loguniform('learning_rate', 1e-6, 1e+2)\n",
    "\n",
    "    #We need to create the optimizer (and any learning rate schedule) outside of the train_network call so that the same optimizer is re-used between epochs\n",
    "    optimizer = torch.optim.AdamW(fc_model.parameters(), lr=eta_global)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs//3)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "    \n",
    "        results = train_network(fc_model, loss_func, t_loader, val_loader=v_loader,\n",
    "                                         epochs=1, optimizer=optimizer, lr_schedule=scheduler,\n",
    "                                         score_funcs={'Accuracy': accuracy_score}, device=device, \n",
    "                                         disable_tqdm=True)\n",
    "        cur_accuracy = results['val Accuracy'].iloc[-1]\n",
    "        trial.report(cur_accuracy, epoch)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return cur_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study2 = optuna.create_study(direction='maximize')\n",
    "study2.optimize(objectivePrunable, n_trials=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
