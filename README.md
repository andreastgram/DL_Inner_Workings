# DL_Inner_Workings
A repository taking a deeper dive into deep learning concepts and building blocks. Use for reference and practice. 

#### Part 1, “Foundational methods,” has six chapters:

* Chapter 1 discusses PyTorch and the basics of how it works, showing you how to use the framework.

* Chapter 2 covers the most basic type of neural network—a fully connected network—and how to write code to train arbitrary networks in PyTorch. This includes a walk-through showing how a fully connected network is related to linear models.

* Chapter 3 introduces convolutions and how they enable convolutional neural networks that have dominated image-based deep learning.

* Chapter 4 introduces recurrent neural networks, how they encode sequential information, and how they can be used for text classification problems.

* Chapter 5 introduces newer training techniques that can be applied to any neural network to obtain higher accuracy in less time, and explains how they can achieve this goal.

* Chapter 6 develops the modern design patterns in common use today, bringing your knowledge of designing a neural network into the modern age.

#### Part 2, “Building advanced networks,” has eight chapters:

* Chapter 7 introduces autoencoding as a technique for training a neural network without labeled data, allowing unsupervised learning.

* Chapter 8 introduces image segmentation and object detection as two techniques you can use to find multiple items within an image.

* Chapter 9 develops the generative adversarial network, an unsupervised approach that can produce synthetic data and is the foundation of many modern image-alteration and deep-fake techniques.

* Chapter 10 teaches you how to implement an attention mechanism, one of the most important recent advances in network priors. Attention mechanisms allow deep networks to selectively ignore irrelevant or unimportant parts of the input.

* Chapter 11 uses attention to build the seminal Seq2Seq model and shows how to build an English-to-French translator using the same approaches deployed in production systems.

* Chapter 12 introduces a new strategy for avoiding recurrent networks (due to their disadvantages) by rethinking how networks are designed. This includes the transformer architecture, a foundation of the best current tools for natural language processing.

* Chapter 13 covers transfer learning, an approach that uses networks trained on one dataset to improve performance on another. This allows using less labeled data, making it one of the most useful tricks in real-world work.

* Chapter 14 ends the book by revisiting some of the most fundamental components of a modern neural network and teaching you three recently published techniques that most practitioners still aren’t aware of to build better models.