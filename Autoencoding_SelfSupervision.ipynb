{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "\n",
    "import idlmam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available(): \n",
    "    device = torch.device(\"mps\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 28 * 28\n",
    "n = 2 \n",
    "C = 1 \n",
    "classes = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeLinear(nn.Module):                    \n",
    "    def __init__(self, linearLayer, bias=True):\n",
    "\n",
    "        \"\"\" \n",
    "        linearLayer: is the layer that we want to use the transpose of to \n",
    "        ➥ produce the output of this layer. So the Linear layer represents \n",
    "        ➥ W, and this layer represents W^T. This is accomplished via \n",
    "        ➥ weight sharing by reusing the weights of linearLayer \n",
    "        bias: if True, we will create a new bias term b that is learned \n",
    "        separately from what is in \n",
    "        linearLayer. If false, we will not use any bias vector. \n",
    "        \"\"\" \n",
    "\n",
    "        super().__init__() \n",
    "        self.weight = linearLayer.weight                \n",
    "\n",
    "        if bias: \n",
    "            self.bias = nn.Parameter(torch.Tensor(\n",
    "                                    linearLayer.weight.shape[1]))           \n",
    "\n",
    "\n",
    "        else:\n",
    "            self.register_parameter('bias', None)      \n",
    "\n",
    "\n",
    "    def forward(self, x):                              \n",
    "        return F.linear(x, self.weight.t(), self.bias) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearLayer = nn.Linear(D, n, bias=False)\n",
    "\n",
    "pca_encoder = nn.Sequential(\n",
    "    nn.Flatten(), linearLayer,\n",
    ")\n",
    "\n",
    "pca_decoder = nn.Sequential(\n",
    "    TransposeLinear(linearLayer, bias=False),\n",
    "    idlmam.View((-1, 1, 28, 28)),\n",
    ")\n",
    "\n",
    "pca_model = nn.Sequential(\n",
    "    pca_encoder, \n",
    "    pca_decoder\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it truly PCA, we need to add the WW⊤ = I constraint, orthogonality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.orthogonal_(linearLayer.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
